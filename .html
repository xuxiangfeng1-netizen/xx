<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hierarchical Text-Conditional Image Generation with CLIP Latents (unCLIP)</title>
  <style>
    body {
      font-family: "Helvetica", "Arial", sans-serif;
      line-height: 1.6;
      margin: 40px auto;
      max-width: 900px;
      color: #222;
      background-color: #fafafa;
    }
    h1, h2, h3 {
      color: #0a4d91;
    }
    img {
      display: block;
      margin: 20px auto;
      max-width: 80%;
      border-radius: 10px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.2);
    }
    hr {
      border: 0;
      height: 1px;
      background: #ddd;
      margin: 40px 0;
    }
    .source {
      font-size: 0.9em;
      color: #666;
      text-align: center;
    }
    .table-container {
      overflow-x: auto;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: center;
    }
    th {
      background-color: #f2f2f2;
    }
  </style>
</head>
<body>

<h1>Hierarchical Text-Conditional Image Generation with CLIP Latents (unCLIP)</h1>
<p><b>Author:</b> Xiangfeng Xu</p>
<p><b>Course:</b> Deep Learning / AI Paper Blog Assignment</p>
<p><b>Date:</b> October 2025</p>

<hr>

<h2>1. Research Background</h2>
<p>
In recent years, artificial intelligence has achieved remarkable progress in image generation.
Traditional GAN-based models perform well in image synthesis but often lack semantic consistency and diversity.
The CLIP model (Contrastive Language–Image Pretraining) from OpenAI learns through contrastive training on large image–text datasets,
providing strong semantic understanding and zero-shot generalization capabilities.
The unCLIP framework combines CLIP’s representational power with Diffusion Models to enable hierarchical, text-guided image generation with enhanced quality, diversity, and interpretability.
</p>

<h2>2. Research Objectives and Innovations</h2>
<p>
The goal of unCLIP is to generate controllable, high-fidelity, semantically consistent, and diverse images conditioned on text.
</p>
<ul>
  <li><b>Hierarchical Generation:</b> A two-stage process where a <i>Prior model</i> maps text into CLIP embeddings, and a <i>Decoder model</i> reconstructs the image.</li>
  <li><b>CLIP Latent Space:</b> Provides a semantic bridge between language and vision.</li>
  <li><b>Diffusion Prior:</b> Improves efficiency and visual quality over autoregressive models.</li>
  <li><b>Text-Guided Editing (Text Diffs):</b> Enables fine-grained semantic manipulation through language prompts.</li>
</ul>

<hr>

<h2>3. Model Architecture</h2>
<p>
unCLIP decomposes the text-to-image task into two learnable parts:
</p>
<ol>
  <li>Text → CLIP Embedding (semantic representation)</li>
  <li>Embedding → Image (visual reconstruction)</li>
</ol>
<p>
This hierarchical structure simplifies training and enhances controllability.
</p>

<img src="https://raw.githubusercontent.com/xuxiangfeng1-netizen/XUXIANGFENG.github.io/main/me.jpg" alt="unCLIP Model Overview">
<p class="source">Figure 1: unCLIP model overview. Source: Ramesh et al., 2022.</p>

<hr>

<h2>4. Prior Model and Decoder</h2>
<p>
The <b>Prior model</b> transforms text embeddings into CLIP image embeddings using either:
</p>
<ul>
  <li>Autoregressive Prior (accurate but slow)</li>
  <li>Diffusion Prior (efficient and high-quality)</li>
</ul>
<p>
The <b>Decoder</b> then employs a diffusion process to progressively denoise and generate the final image, conditioned on the CLIP embedding.
</p>

<hr>

<h2>5. Training Highlights</h2>
<ul>
  <li><b>Dataset:</b> 650M text–image pairs from the Internet.</li>
  <li><b>Classifier-Free Guidance:</b> Randomly drops conditioning to balance fidelity and diversity.</li>
  <li><b>Hierarchical Upsampling:</b> From 64×64 → 256×256 → 1024×1024 resolutions.</li>
  <li><b>PCA Optimization:</b> Reduces embedding dimensionality while retaining semantic richness.</li>
</ul>

<hr>

<h2>6. Experimental Results</h2>
<p>
unCLIP outperforms earlier models like GLIDE and DALL·E on benchmarks such as MS-COCO 256×256.
It achieves a FID score of 10.39, compared to GLIDE’s 12.24 and DALL·E’s 28.
Human evaluation confirms that unCLIP balances photorealism and creativity effectively.
</p>

<img src="https://cdn.openai.com/research-covers/unclip/unclip-examples.jpg" alt="unCLIP text-to-image results">
<p class="source">Figure 2: unCLIP-generated examples (Source: Ramesh et al., 2022).</p>

<div class="table-container">
<table>
  <tr><th>Model</th><th>Photorealism</th><th>Caption Similarity</th><th>Diversity</th></tr>
  <tr><td>GLIDE</td><td>52.9%</td><td>58.9%</td><td>37.4%</td></tr>
  <tr><td>unCLIP (AR Prior)</td><td>47.1%</td><td>41.1%</td><td>62.6%</td></tr>
  <tr><td>unCLIP (Diffusion Prior)</td><td>48.9%</td><td>45.3%</td><td>70.5%</td></tr>
</table>
</div>

<hr>

<h2>7. Innovations Summary</h2>
<table>
  <tr><th>Innovation</th><th>Technique</th><th>Effect</th></tr>
  <tr><td>Hierarchical Generation</td><td>Text → Embedding → Image</td><td>Improves semantic consistency</td></tr>
  <tr><td>Diffusion Prior</td><td>Continuous latent modeling</td><td>Boosts quality and speed</td></tr>
  <tr><td>Latent Visualization</td><td>CLIP decoding</td><td>Enhances interpretability</td></tr>
  <tr><td>Classifier-Free Guidance</td><td>Conditional dropout</td><td>Balances fidelity and diversity</td></tr>
</table>

<hr>

<h2>8. Limitations and Risks</h2>
<ul>
  <li>Attribute confusion (e.g., “a red cube on a blue cube”)</li>
  <li>Distorted text rendering</li>
  <li>Loss of detail in complex scenes</li>
  <li>Ethical concerns around realism and misinformation</li>
</ul>

<img src="https://cdn.openai.com/research-covers/unclip/unclip-failures.jpg" alt="unCLIP limitations">
<p class="source">Figure 3: unCLIP limitations. Source: Ramesh et al., 2022.</p>

<hr>

<h2>9. Applications and Future Work</h2>
<p>
unCLIP has been integrated into OpenAI’s DALL·E 2, demonstrating its power in art, media, and content generation.
Future directions include:
</p>
<ul>
  <li>Higher base resolutions</li>
  <li>Improved attribute–object binding</li>
  <li>More controllable latent editing</li>
  <li>Ethical safeguards and watermarking</li>
</ul>

<hr>

<h2>10. Conclusion</h2>
<p>
The unCLIP framework bridges semantic understanding and high-fidelity image generation.
By leveraging CLIP latent representations and diffusion modeling, it achieves a new balance of accuracy, diversity, and interpretability—surpassing GLIDE and DALL·E in both creativity and visual quality.
As generative models become more realistic, continued attention to ethical transparency and safety remains crucial.
</p>

<hr>
<p class="source">
Source: Ramesh et al., <i>Hierarchical Text-Conditional Image Generation with CLIP Latents</i>, arXiv:2204.06125 (2022)
</p>

</body>
</html>
